{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f530f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample:\n",
      "                                             reviews  sentiments\n",
      "0  I bought this belt for my daughter in-law for ...           1\n",
      "1  The size was perfect and so was the color.  It...           1\n",
      "2  Fits and feels good, esp. for doing a swim rac...           1\n",
      "3  These socks are absolutely the best. I take pi...           1\n",
      "4  Thank you so much for the speedy delivery they...           1\n",
      "\n",
      "Testing Data Sample:\n",
      "                                             reviews\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...\n",
      "1  I dare say these are just about the sexiest th...\n",
      "2  everything about the transaction (price, deliv...\n",
      "3  Not bad for just a shirt.  Very durable, and m...\n",
      "4  These are truly wrinkle free and longer than t...\n",
      "\n",
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7401 entries, 0 to 7400\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   reviews     7401 non-null   object\n",
      " 1   sentiments  7401 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 115.8+ KB\n",
      "None\n",
      "\n",
      "Testing Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1851 entries, 0 to 1850\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   reviews  1851 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 14.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "# Inspect data\n",
    "print(\"Training Data Sample:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nTesting Data Sample:\")\n",
    "print(test_data.head())\n",
    "\n",
    "# Check data structure\n",
    "print(\"\\nTraining Data Info:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nTesting Data Info:\")\n",
    "print(test_data.info())\n",
    "\n",
    "# Access reviews and sentiments\n",
    "reviews = train_data['reviews']\n",
    "sentiments = train_data['sentiments']\n",
    "test_reviews = test_data['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050cab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  \\\n",
      "0  I bought this belt for my daughter in-law for ...   \n",
      "1  The size was perfect and so was the color.  It...   \n",
      "2  Fits and feels good, esp. for doing a swim rac...   \n",
      "3  These socks are absolutely the best. I take pi...   \n",
      "4  Thank you so much for the speedy delivery they...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                             reviews  \\\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...   \n",
      "1  I dare say these are just about the sexiest th...   \n",
      "2  everything about the transaction (price, deliv...   \n",
      "3  Not bad for just a shirt.  Very durable, and m...   \n",
      "4  These are truly wrinkle free and longer than t...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove non-alphabet characters\n",
    "    return text\n",
    "\n",
    "train_data['cleaned_reviews'] = train_data['reviews'].apply(clean_text)\n",
    "test_data['cleaned_reviews'] = test_data['reviews'].apply(clean_text)\n",
    "print(train_data[['reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12823ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\darre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [i, bought, this, belt, for, my, daughter, inl...   \n",
      "1  [the, size, was, perfect, and, so, was, the, c...   \n",
      "2  [fits, and, feels, good, esp, for, doing, a, s...   \n",
      "3  [these, socks, are, absolutely, the, best, i, ...   \n",
      "4  [thank, you, so, much, for, the, speedy, deliv...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [i, bought, sleepers, sleeper, had, holes, in,...   \n",
      "1  [i, dare, say, these, are, just, about, the, s...   \n",
      "2  [everything, about, the, transaction, price, d...   \n",
      "3  [not, bad, for, just, a, shirt, very, durable,...   \n",
      "4  [these, are, truly, wrinkle, free, and, longer...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download tokenizer\n",
    "\n",
    "train_data['tokenized_reviews'] = train_data['cleaned_reviews'].apply(word_tokenize)\n",
    "test_data['tokenized_reviews'] = test_data['cleaned_reviews'].apply(word_tokenize)\n",
    "print(train_data[['tokenized_reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a38a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\darre\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [bought, belt, daughter, inlaw, christmas, loved]   \n",
      "1    [size, perfect, color, looked, like, web, page]   \n",
      "2  [fits, feels, good, esp, swim, race, highly, r...   \n",
      "3  [socks, absolutely, best, take, pilates, class...   \n",
      "4  [thank, much, speedy, delivery, came, time, re...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [bought, sleepers, sleeper, holes, arm, pit, a...   \n",
      "1  [dare, say, sexiest, things, ive, ever, worn, ...   \n",
      "2  [everything, transaction, price, delivery, tim...   \n",
      "3  [bad, shirt, durable, matched, teams, colors, ...   \n",
      "4  [truly, wrinkle, free, longer, average, womans...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_data['tokenized_reviews'] = train_data['tokenized_reviews'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "test_data['tokenized_reviews'] = test_data['tokenized_reviews'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "print(train_data[['tokenized_reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8cacd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\darre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\darre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      tokenized_reviews  \\\n",
      "0     [bought, belt, daughter, inlaw, christmas, loved]   \n",
      "1       [size, perfect, color, looked, like, web, page]   \n",
      "2     [fits, feels, good, esp, swim, race, highly, r...   \n",
      "3     [socks, absolutely, best, take, pilates, class...   \n",
      "4     [thank, much, speedy, delivery, came, time, re...   \n",
      "...                                                 ...   \n",
      "7396  [bought, shirts, black, medium, wear, daily, b...   \n",
      "7397  [first, thought, scarf, might, good, quality, ...   \n",
      "7398  [picky, comes, bras, want, something, support,...   \n",
      "7399  [jacket, wind, water, resistant, waterproof, s...   \n",
      "7400  [extremely, confortable, material, soft, cotto...   \n",
      "\n",
      "                                     lemmatized_reviews  \n",
      "0            bought belt daughter inlaw christmas loved  \n",
      "1               size perfect color looked like web page  \n",
      "2     fit feel good esp swim race highly recommend c...  \n",
      "3     sock absolutely best take pilate class hot foo...  \n",
      "4     thank much speedy delivery came time rehearsal...  \n",
      "...                                                 ...  \n",
      "7396  bought shirt black medium wear daily basis dis...  \n",
      "7397  first thought scarf might good quality since c...  \n",
      "7398  picky come bra want something support comforta...  \n",
      "7399  jacket wind water resistant waterproof soked f...  \n",
      "7400  extremely confortable material soft cotton pou...  \n",
      "\n",
      "[7401 rows x 2 columns]\n",
      "                                      tokenized_reviews  \\\n",
      "0     [bought, sleepers, sleeper, holes, arm, pit, a...   \n",
      "1     [dare, say, sexiest, things, ive, ever, worn, ...   \n",
      "2     [everything, transaction, price, delivery, tim...   \n",
      "3     [bad, shirt, durable, matched, teams, colors, ...   \n",
      "4     [truly, wrinkle, free, longer, average, womans...   \n",
      "...                                                 ...   \n",
      "1846  [stylish, nice, perfect, wear, pair, cutoff, l...   \n",
      "1847  [bought, longsleeved, colored, shirts, child, ...   \n",
      "1848  [really, cute, sexy, make, nice, valentines, d...   \n",
      "1849  [shoers, daughter, loves, long, happy, happy, ...   \n",
      "1850  [umbrellas, handle, light, rain, well, leave, ...   \n",
      "\n",
      "                                     lemmatized_reviews  \n",
      "0     bought sleeper sleeper hole arm pit area sleep...  \n",
      "1     dare say sexiest thing ive ever worn oh ive gs...  \n",
      "2     everything transaction price delivery time qua...  \n",
      "3     bad shirt durable matched team color perfectly...  \n",
      "4     truly wrinkle free longer average woman botton...  \n",
      "...                                                 ...  \n",
      "1846  stylish nice perfect wear pair cutoff levi cut...  \n",
      "1847  bought longsleeved colored shirt child wear ro...  \n",
      "1848  really cute sexy make nice valentine day prese...  \n",
      "1849  shoers daughter love long happy happy adidas d...  \n",
      "1850  umbrella handle light rain well leave wet pant...  \n",
      "\n",
      "[1851 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources (if you haven't already)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    # Lemmatize each word and join them back into a single string\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the lemmatization function to the 'reviews' column\n",
    "train_data['lemmatized_reviews'] = train_data['tokenized_reviews'].apply(lemmatize_text)\n",
    "test_data['lemmatized_reviews'] = test_data['tokenized_reviews'].apply(lemmatize_text)\n",
    "\n",
    "# Check the results\n",
    "print(train_data[['tokenized_reviews', 'lemmatized_reviews']])\n",
    "print(test_data[['tokenized_reviews', 'lemmatized_reviews']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4fd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train_data['lemmatized_reviews'] and test_data['lemmatized_reviews'] for model training & prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
