{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3dd078e5-1da3-4feb-8b61-fb38a0d71a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1855c06-b34c-40d4-b0db-986773efe1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec embeddings\n",
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a4f3678-124c-41fd-b6b1-e2846bdce774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                reviews  sentiments\n",
      "0     I bought this belt for my daughter in-law for ...           1\n",
      "1     The size was perfect and so was the color.  It...           1\n",
      "2     Fits and feels good, esp. for doing a swim rac...           1\n",
      "3     These socks are absolutely the best. I take pi...           1\n",
      "4     Thank you so much for the speedy delivery they...           1\n",
      "...                                                 ...         ...\n",
      "7180  I bought these shirts (black, medium) to wear ...           0\n",
      "7181  At first, I thought this scarf might not be th...           1\n",
      "7182  I am very picky when it comes to bras.  I want...           1\n",
      "7183  This jacket is wind and water resistant, but n...           0\n",
      "7184  These are extremely confortable. The material ...           1\n",
      "\n",
      "[7185 rows x 2 columns]\n",
      "0       False\n",
      "1       False\n",
      "2       False\n",
      "3       False\n",
      "4       False\n",
      "        ...  \n",
      "7180    False\n",
      "7181    False\n",
      "7182    False\n",
      "7183    False\n",
      "7184    False\n",
      "Length: 7185, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "#Drop Duplicates\n",
    "train_data = train_data.drop_duplicates(subset='reviews').reset_index(drop=True)\n",
    "print(train_data)\n",
    "\n",
    "#Find Duplicates\n",
    "findDuplicate = train_data.duplicated()\n",
    "print(findDuplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b01bdf2b-ff53-4329-a229-8d7c13f1949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  \\\n",
      "0  I bought this belt for my daughter in-law for ...   \n",
      "1  The size was perfect and so was the color.  It...   \n",
      "2  Fits and feels good, esp. for doing a swim rac...   \n",
      "3  These socks are absolutely the best. I take pi...   \n",
      "4  Thank you so much for the speedy delivery they...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                             reviews  \\\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...   \n",
      "1  I dare say these are just about the sexiest th...   \n",
      "2  everything about the transaction (price, deliv...   \n",
      "3  Not bad for just a shirt.  Very durable, and m...   \n",
      "4  These are truly wrinkle free and longer than t...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove non-alphabet characters\n",
    "    return text\n",
    "\n",
    "train_data['cleaned_reviews'] = train_data['reviews'].apply(clean_text)\n",
    "test_data['cleaned_reviews'] = test_data['reviews'].apply(clean_text)\n",
    "print(train_data[['reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d7ad8138-1d50-4187-8018-22cc25ebde0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [i, bought, this, belt, for, my, daughter, inl...   \n",
      "1  [the, size, was, perfect, and, so, was, the, c...   \n",
      "2  [fits, and, feels, good, esp, for, doing, a, s...   \n",
      "3  [these, socks, are, absolutely, the, best, i, ...   \n",
      "4  [thank, you, so, much, for, the, speedy, deliv...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [i, bought, sleepers, sleeper, had, holes, in,...   \n",
      "1  [i, dare, say, these, are, just, about, the, s...   \n",
      "2  [everything, about, the, transaction, price, d...   \n",
      "3  [not, bad, for, just, a, shirt, very, durable,...   \n",
      "4  [these, are, truly, wrinkle, free, and, longer...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    }
   ],
   "source": [
    "#tokenize reviews\n",
    "train_data['tokenized_reviews'] = train_data['cleaned_reviews'].apply(word_tokenize)\n",
    "test_data['tokenized_reviews'] = test_data['cleaned_reviews'].apply(word_tokenize)\n",
    "print(train_data[['tokenized_reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "72faeb64-6731-459e-b9b2-5156bed1c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [bought, belt, daughter, inlaw, christmas, loved]   \n",
      "1    [size, perfect, color, looked, like, web, page]   \n",
      "2  [fits, feels, good, esp, swim, race, highly, r...   \n",
      "3  [socks, absolutely, best, take, pilates, class...   \n",
      "4  [thank, much, speedy, delivery, came, time, re...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [bought, sleepers, sleeper, holes, arm, pit, a...   \n",
      "1  [dare, say, sexiest, things, ive, ever, worn, ...   \n",
      "2  [everything, transaction, price, delivery, tim...   \n",
      "3  [bad, shirt, durable, matched, teams, colors, ...   \n",
      "4  [truly, wrinkle, free, longer, average, womans...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_data['tokenized_reviews'] = train_data['tokenized_reviews'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "test_data['tokenized_reviews'] = test_data['tokenized_reviews'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "print(train_data[['tokenized_reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "450da0ef-ee92-4324-b40a-718e9ba09080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      tokenized_reviews  \\\n",
      "0     [bought, belt, daughter, inlaw, christmas, loved]   \n",
      "1       [size, perfect, color, looked, like, web, page]   \n",
      "2     [fits, feels, good, esp, swim, race, highly, r...   \n",
      "3     [socks, absolutely, best, take, pilates, class...   \n",
      "4     [thank, much, speedy, delivery, came, time, re...   \n",
      "...                                                 ...   \n",
      "7180  [bought, shirts, black, medium, wear, daily, b...   \n",
      "7181  [first, thought, scarf, might, good, quality, ...   \n",
      "7182  [picky, comes, bras, want, something, support,...   \n",
      "7183  [jacket, wind, water, resistant, waterproof, s...   \n",
      "7184  [extremely, confortable, material, soft, cotto...   \n",
      "\n",
      "                                     lemmatized_reviews  \n",
      "0            bought belt daughter inlaw christmas loved  \n",
      "1               size perfect color looked like web page  \n",
      "2     fit feel good esp swim race highly recommend c...  \n",
      "3     sock absolutely best take pilate class hot foo...  \n",
      "4     thank much speedy delivery came time rehearsal...  \n",
      "...                                                 ...  \n",
      "7180  bought shirt black medium wear daily basis dis...  \n",
      "7181  first thought scarf might good quality since c...  \n",
      "7182  picky come bra want something support comforta...  \n",
      "7183  jacket wind water resistant waterproof soked f...  \n",
      "7184  extremely confortable material soft cotton pou...  \n",
      "\n",
      "[7185 rows x 2 columns]\n",
      "                                      tokenized_reviews  \\\n",
      "0     [bought, sleepers, sleeper, holes, arm, pit, a...   \n",
      "1     [dare, say, sexiest, things, ive, ever, worn, ...   \n",
      "2     [everything, transaction, price, delivery, tim...   \n",
      "3     [bad, shirt, durable, matched, teams, colors, ...   \n",
      "4     [truly, wrinkle, free, longer, average, womans...   \n",
      "...                                                 ...   \n",
      "1846  [stylish, nice, perfect, wear, pair, cutoff, l...   \n",
      "1847  [bought, longsleeved, colored, shirts, child, ...   \n",
      "1848  [really, cute, sexy, make, nice, valentines, d...   \n",
      "1849  [shoers, daughter, loves, long, happy, happy, ...   \n",
      "1850  [umbrellas, handle, light, rain, well, leave, ...   \n",
      "\n",
      "                                     lemmatized_reviews  \n",
      "0     bought sleeper sleeper hole arm pit area sleep...  \n",
      "1     dare say sexiest thing ive ever worn oh ive gs...  \n",
      "2     everything transaction price delivery time qua...  \n",
      "3     bad shirt durable matched team color perfectly...  \n",
      "4     truly wrinkle free longer average woman botton...  \n",
      "...                                                 ...  \n",
      "1846  stylish nice perfect wear pair cutoff levi cut...  \n",
      "1847  bought longsleeved colored shirt child wear ro...  \n",
      "1848  really cute sexy make nice valentine day prese...  \n",
      "1849  shoers daughter love long happy happy adidas d...  \n",
      "1850  umbrella handle light rain well leave wet pant...  \n",
      "\n",
      "[1851 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK resources (if you haven't already)\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    # Lemmatize each word and join them back into a single string\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply the lemmatization function to the 'reviews' column\n",
    "train_data['lemmatized_reviews'] = train_data['tokenized_reviews'].apply(lemmatize_text)\n",
    "test_data['lemmatized_reviews'] = test_data['tokenized_reviews'].apply(lemmatize_text)\n",
    "\n",
    "# Check the results\n",
    "print(train_data[['tokenized_reviews', 'lemmatized_reviews']])\n",
    "print(test_data[['tokenized_reviews', 'lemmatized_reviews']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59ac3915-2fb5-4815-8fbd-d9bbedc91813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.8364 - loss: 0.4769 - val_accuracy: 0.8447 - val_loss: 0.3747\n",
      "Epoch 2/5\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.8526 - loss: 0.3709 - val_accuracy: 0.8614 - val_loss: 0.3118\n",
      "Epoch 3/5\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8808 - loss: 0.2903 - val_accuracy: 0.8687 - val_loss: 0.3003\n",
      "Epoch 4/5\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.8937 - loss: 0.2583 - val_accuracy: 0.8765 - val_loss: 0.2846\n",
      "Epoch 5/5\n",
      "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9124 - loss: 0.2193 - val_accuracy: 0.8742 - val_loss: 0.3107\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "                                             reviews  predicted_sentiments\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...                     0\n",
      "1  I dare say these are just about the sexiest th...                     1\n",
      "2  everything about the transaction (price, deliv...                     1\n",
      "3  Not bad for just a shirt.  Very durable, and m...                     1\n",
      "4  These are truly wrinkle free and longer than t...                     1\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "x, y = (train_data['reviews'].values, train_data['sentiments'].values)\n",
    "\n",
    "# Tokenize and pad the reviews\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(x)\n",
    "x_sequence = tokenizer.texts_to_sequences(x)\n",
    "x_padding = pad_sequences(x_sequence, maxlen=32, padding='post')\n",
    "\n",
    "# Create embedding matrix using Word2Vec\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map the words in our tokenizer's vocabulary to Word2Vec embeddings\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec.key_to_index:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "# Split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_padding, y, test_size=0.25, random_state=1)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer with Word2Vec weights, non-trainable\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "# LSTM layer (captures sequential data)\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "# Dense layer with ReLU activation\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout layer to reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with sigmoid activation for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model performance on the test data\n",
    "x_test_sequence = tokenizer.texts_to_sequences(test_data['reviews'])\n",
    "x_test_padding = pad_sequences(x_test_sequence, maxlen=32, padding='post')\n",
    "y_pred = (model.predict(x_test_padding) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Add predictions to test_df\n",
    "test_data['predicted_sentiments'] = y_pred\n",
    "\n",
    "# Display first few predictions\n",
    "print(test_data[['reviews', 'predicted_sentiments']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "22056f4a-e1e5-41ec-9887-616f82350ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a submission file\n",
    "submission_data = test_data[['reviews', 'predicted_sentiments']]\n",
    "submission_data.to_csv('Results_RNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8c796-49ba-4856-a7b2-215e45a0b5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
