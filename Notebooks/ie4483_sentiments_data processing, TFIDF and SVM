{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0149ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample:\n",
      "                                             reviews  sentiments\n",
      "0  I bought this belt for my daughter in-law for ...           1\n",
      "1  The size was perfect and so was the color.  It...           1\n",
      "2  Fits and feels good, esp. for doing a swim rac...           1\n",
      "3  These socks are absolutely the best. I take pi...           1\n",
      "4  Thank you so much for the speedy delivery they...           1\n",
      "\n",
      "Testing Data Sample:\n",
      "                                             reviews\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...\n",
      "1  I dare say these are just about the sexiest th...\n",
      "2  everything about the transaction (price, deliv...\n",
      "3  Not bad for just a shirt.  Very durable, and m...\n",
      "4  These are truly wrinkle free and longer than t...\n",
      "\n",
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7401 entries, 0 to 7400\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   reviews     7401 non-null   object\n",
      " 1   sentiments  7401 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 115.8+ KB\n",
      "None\n",
      "\n",
      "Testing Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1851 entries, 0 to 1850\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   reviews  1851 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 14.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "# Inspect data\n",
    "print(\"Training Data Sample:\")\n",
    "print(train_data.head())\n",
    "print(\"\\nTesting Data Sample:\")\n",
    "print(test_data.head())\n",
    "\n",
    "# Check data structure\n",
    "print(\"\\nTraining Data Info:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nTesting Data Info:\")\n",
    "print(test_data.info())\n",
    "\n",
    "# Access reviews and sentiments\n",
    "reviews = train_data['reviews']\n",
    "sentiments = train_data['sentiments']\n",
    "test_reviews = test_data['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6bd117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  \\\n",
      "0  I bought this belt for my daughter in-law for ...   \n",
      "1  The size was perfect and so was the color.  It...   \n",
      "2  Fits and feels good, esp. for doing a swim rac...   \n",
      "3  These socks are absolutely the best. I take pi...   \n",
      "4  Thank you so much for the speedy delivery they...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                             reviews  \\\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...   \n",
      "1  I dare say these are just about the sexiest th...   \n",
      "2  everything about the transaction (price, deliv...   \n",
      "3  Not bad for just a shirt.  Very durable, and m...   \n",
      "4  These are truly wrinkle free and longer than t...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # convert to lowercase\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove non-alphabet characters\n",
    "    return text\n",
    "\n",
    "train_data['cleaned_reviews'] = train_data['reviews'].apply(clean_text)\n",
    "test_data['cleaned_reviews'] = test_data['reviews'].apply(clean_text)\n",
    "print(train_data[['reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "846ca4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [i, bought, this, belt, for, my, daughter, inl...   \n",
      "1  [the, size, was, perfect, and, so, was, the, c...   \n",
      "2  [fits, and, feels, good, esp, for, doing, a, s...   \n",
      "3  [these, socks, are, absolutely, the, best, i, ...   \n",
      "4  [thank, you, so, much, for, the, speedy, deliv...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [i, bought, sleepers, sleeper, had, holes, in,...   \n",
      "1  [i, dare, say, these, are, just, about, the, s...   \n",
      "2  [everything, about, the, transaction, price, d...   \n",
      "3  [not, bad, for, just, a, shirt, very, durable,...   \n",
      "4  [these, are, truly, wrinkle, free, and, longer...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download tokenizer\n",
    "\n",
    "train_data['tokenized_reviews'] = train_data['cleaned_reviews'].apply(word_tokenize)\n",
    "test_data['tokenized_reviews'] = test_data['cleaned_reviews'].apply(word_tokenize)\n",
    "print(train_data[['tokenized_reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33708b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [bought, belt, daughter, inlaw, christmas, loved]   \n",
      "1    [size, perfect, color, looked, like, web, page]   \n",
      "2  [fits, feels, good, esp, swim, race, highly, r...   \n",
      "3  [socks, absolutely, best, take, pilates, class...   \n",
      "4  [thank, much, speedy, delivery, came, time, re...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought this belt for my daughter inlaw for c...  \n",
      "1  the size was perfect and so was the color  it ...  \n",
      "2  fits and feels good esp for doing a swim race ...  \n",
      "3  these socks are absolutely the best i take pil...  \n",
      "4  thank you so much for the speedy delivery they...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [bought, sleepers, sleeper, holes, arm, pit, a...   \n",
      "1  [dare, say, sexiest, things, ive, ever, worn, ...   \n",
      "2  [everything, transaction, price, delivery, tim...   \n",
      "3  [bad, shirt, durable, matched, teams, colors, ...   \n",
      "4  [truly, wrinkle, free, longer, average, womans...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  i bought  sleepers  sleeper had holes in the a...  \n",
      "1  i dare say these are just about the sexiest th...  \n",
      "2  everything about the transaction price deliver...  \n",
      "3  not bad for just a shirt  very durable and mat...  \n",
      "4  these are truly wrinkle free and longer than t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "train_data['tokenized_reviews'] = train_data['tokenized_reviews'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "test_data['tokenized_reviews'] = test_data['tokenized_reviews'].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "print(train_data[['tokenized_reviews', 'cleaned_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'cleaned_reviews']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3673dd46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\maple\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   tokenized_reviews  \\\n",
      "0  [bought, belt, daughter, inlaw, christmas, loved]   \n",
      "1    [size, perfect, color, looked, like, web, page]   \n",
      "2  [fits, feels, good, esp, swim, race, highly, r...   \n",
      "3  [socks, absolutely, best, take, pilates, class...   \n",
      "4  [thank, much, speedy, delivery, came, time, re...   \n",
      "\n",
      "                                  lemmatized_reviews  \n",
      "0  [ 'bought ' , 'belt ' , 'daughter ' , 'inlaw '...  \n",
      "1  [ 'size ' , 'perfect ' , 'color ' , 'looked ' ...  \n",
      "2  [ 'fits ' , 'feels ' , 'good ' , 'esp ' , 'swi...  \n",
      "3  [ 'socks ' , 'absolutely ' , 'best ' , 'take '...  \n",
      "4  [ 'thank ' , 'much ' , 'speedy ' , 'delivery '...  \n",
      "                                   tokenized_reviews  \\\n",
      "0  [bought, sleepers, sleeper, holes, arm, pit, a...   \n",
      "1  [dare, say, sexiest, things, ive, ever, worn, ...   \n",
      "2  [everything, transaction, price, delivery, tim...   \n",
      "3  [bad, shirt, durable, matched, teams, colors, ...   \n",
      "4  [truly, wrinkle, free, longer, average, womans...   \n",
      "\n",
      "                                  lemmatized_reviews  \n",
      "0  [ 'bought ' , 'sleepers ' , 'sleeper ' , 'hole...  \n",
      "1  [ 'dare ' , 'say ' , 'sexiest ' , 'things ' , ...  \n",
      "2  [ 'everything ' , 'transaction ' , 'price ' , ...  \n",
      "3  [ 'bad ' , 'shirt ' , 'durable ' , 'matched ' ...  \n",
      "4  [ 'truly ' , 'wrinkle ' , 'free ' , 'longer ' ...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the lemmatization function\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Return empty string if text is not a valid string\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# Apply the lemmatization function to your DataFrame, handling missing values\n",
    "test_data['lemmatized_reviews'] = test_data['tokenized_reviews'].apply(lambda x: lemmatize_text(str(x)))\n",
    "train_data['lemmatized_reviews'] = train_data['tokenized_reviews'].apply(lambda x: lemmatize_text(str(x)))\n",
    "\n",
    "# Check the results\n",
    "print(train_data[['tokenized_reviews', 'lemmatized_reviews']].head())\n",
    "print(test_data[['tokenized_reviews', 'lemmatized_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "078eba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with C=0.1 and max_features=5000\n",
      "Validation Accuracy: 0.9054692775151925\n",
      "Classification Report on Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       209\n",
      "           1       0.94      0.95      0.95      1272\n",
      "\n",
      "    accuracy                           0.91      1481\n",
      "   macro avg       0.81      0.80      0.80      1481\n",
      "weighted avg       0.90      0.91      0.90      1481\n",
      "\n",
      "Submission file 'submission_C0.1_features5000.csv' created with predictions.\n",
      "\n",
      "Testing with C=10 and max_features=5000\n",
      "Validation Accuracy: 0.9209993247805537\n",
      "Classification Report on Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.47      0.63       209\n",
      "           1       0.92      0.99      0.96      1272\n",
      "\n",
      "    accuracy                           0.92      1481\n",
      "   macro avg       0.93      0.73      0.79      1481\n",
      "weighted avg       0.92      0.92      0.91      1481\n",
      "\n",
      "Submission file 'submission_C10_features5000.csv' created with predictions.\n",
      "\n",
      "Testing with C=1.0 and max_features=7000\n",
      "Validation Accuracy: 0.9216745442268738\n",
      "Classification Report on Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.51      0.65       209\n",
      "           1       0.92      0.99      0.96      1272\n",
      "\n",
      "    accuracy                           0.92      1481\n",
      "   macro avg       0.91      0.75      0.80      1481\n",
      "weighted avg       0.92      0.92      0.91      1481\n",
      "\n",
      "Submission file 'submission_C1.0_features7000.csv' created with predictions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Updated configurations for testing\n",
    "params_to_test = [\n",
    "    {\"C\": 0.1, \"max_features\": 5000},\n",
    "    {\"C\": 10, \"max_features\": 5000},\n",
    "    {\"C\": 1.0, \"max_features\": 7000}  # keep the default C but increase features\n",
    "]\n",
    "\n",
    "# Loop through each configuration\n",
    "for param in params_to_test:\n",
    "    print(f\"\\nTesting with C={param['C']} and max_features={param['max_features']}\")\n",
    "    \n",
    "    # Initialize TF-IDF Vectorizer with the specified max_features\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=param[\"max_features\"], stop_words='english', ngram_range=(1,2))\n",
    "    \n",
    "    # Fit and transform the train data\n",
    "    X_train_full = tfidf_vectorizer.fit_transform(train_data['lemmatized_reviews'])\n",
    "    X_test = tfidf_vectorizer.transform(test_data['lemmatized_reviews'])\n",
    "    \n",
    "    # Split the training data into a training and validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train the SVM model with the specified C and class balancing\n",
    "    svm_model = SVC(C=param['C'], class_weight='balanced')\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_val_pred = svm_model.predict(X_val)\n",
    "    \n",
    "    # Evaluate model performance on the validation set\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    report = classification_report(y_val, y_val_pred)\n",
    "    print(\"Validation Accuracy:\", accuracy)\n",
    "    print(\"Classification Report on Validation Set:\\n\", report)\n",
    "    \n",
    "    # Predict on the test set (for final submission if needed)\n",
    "    y_test_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Save the test set predictions to a CSV file (optional for each configuration)\n",
    "    submission = pd.DataFrame({\"review\": test_data['reviews'], \"predicted_sentiment\": y_test_pred})\n",
    "    submission_filename = f\"submission_C{param['C']}_features{param['max_features']}.csv\"\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"Submission file '{submission_filename}' created with predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d102c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
